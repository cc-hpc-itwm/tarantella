.. _quick-start-label:

Quick start
===========

This section explains how to get started using Tarantella to distributetly
train an existing TensorFlow 2/Keras model.
First we will examine what changes to your code have to be made, before we will look into
the execution of your script with ``tnt_run`` on the command line.
Finally, we will state in more detail what features Tarantella currently supports and
what important points need to be taken into account when using Tarantella.

Code example: LeNet-5 on MNIST
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

After having :ref:`build and installed <installation-label>` Tarantella
we are ready to add distributed training support to an existing TensorFlow 2/Keras model.
We will first illustrate all the necessary steps, using the well-know example of
**LeNet-5** on the **MNIST** dataset. Although, this is not necessarily a good use case
to take full advantage of Tarantella's capabilities, it will allow you to simply
copy-paste the code snippets and try them out, even on your laptop.
  
**Let's get started!**

.. literalinclude:: quick_start_model.py
   :language: Python
   :linenos:
   :emphasize-lines: 3,9,13

As you can see from the marked lines in the code snippet above, in the simplest case
you only need to add *3 lines of code* to train your model distributedly using Tarantella!
Let us go through the code in some more detail, in order to understand what is going on.

First we need to import the Tarantella library:

.. code-block:: Python

   import tarantella as tnt

Having done that we need to initialise the library (which will setup the communication infrastructure):

.. code-block:: Python

   tnt.init()

Note that this should be done before executing any other code. Next, we need to wrap the
``keras.Model`` object, generated by ``lenet5_model_generator()``, into a ``tnt.Model`` object:

.. code-block:: Python

   model = tnt.Model(lenet5_model_generator())

**That's it!**

All the necessary steps to distribute training and datasets will now automatically be handled by Tarantella.
In particular, we run ``model.compile`` on our ``model`` to generate a (distributed) compute graph,
the same way we did with Keras before.

Next, we load the MNIST data for training, validation and testing, and
create ``Dataset`` s from it. Note that we ``batch`` the dataset for training.
This will guarantee that Tarantella is able to distribute the data later on in the correct way.
Also note that the ``batch_size`` used here, is the same as for the original model,
that is the global batch size.  For details concerning local and global batch sizes have a look
:ref:`here <global-vs-local-batch-size-label>`.

Now we are able to train our ``model`` using ``model.fit``. This, as well, is identical
to the standard Keras interface. Note, however, that Tarantella is taking care of proper
distribution of the ``train_dataset`` in the background. All the possibilities of how to
feed datasets to Tarantella are explained in more detail below.

Lastly, we can evaluate the final accuracy of your ``model`` on the ``test_dataset`` using
``model.evaluate``.

Executing your model: ``tnt_run``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Next, let's execute our model distributedly using ``tnt_run`` on the command line.

.. todo::

  * simplest solution
  * machine files
  * logging and how to redirect it
  * all run options

Model checkpointing
^^^^^^^^^^^^^^^^^^^

Storing and loading your trained ``Tarantella.Modell`` is simple.

.. todo::

  * add more description

Using distributed datasets
^^^^^^^^^^^^^^^^^^^^^^^^^^

This section explains what needs to be done in order to use Tarantella's distributed datasets correctly.

There are essentially three distingued ways, in which you can provide your datasets to Tarantella.

.. todo::

  * add more description

Important points
^^^^^^^^^^^^^^^^

There is a number of points you should be aware of when using Tarantella.

.. note::

   ``tnt.init()`` needs to be called **after** ``import tarantella as tnt``, but **before**
   any other statement.

This will make sure, GASPIs communication infrastructure is correctly initialized.

.. note::

   Tarantella only supports one ``Keras.Model`` to be transformed into a ``Tarantella.Model``
   per program.

This should not limit the expressibilty of your models, however. In case you want to
use several DNNs in the same model (e.g. to build a GAN), simply construct several
``Keras.Model`` s and combine them into one by calling them explicitely, as discribed
`here <palce_holder>`_.

.. todo::

  * add link to "Keras.Models as callables"

.. note::

  * Tarantella does not support custom training loops.

Instead of using custom training loops, please use ``Model.fit(...)``.

.. note::

   Tarantella supports all
   `TensorFlow optimizers <place_holder>`_
   with the exception of ???.

Since the ``???`` optimizer does not use batches, it is not supported in Tarantella.
How to use your custom gradient-based optimizer is explained :ref:`here <custom-optimizers-label>`.

.. todo::

  * add name of optimizer that is not supported
  * add link to TF optimizers
