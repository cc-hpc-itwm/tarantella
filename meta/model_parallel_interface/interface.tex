\documentclass{report}
%\documentclass[a4paper,10pt]{scrartcl}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm}
%\usepackage{dsfont}
%\usepackage{listings}

\usepackage{amssymb}
\usepackage{color}
\usepackage{lscape}

\usepackage{array}
\usepackage{longtable}
\usepackage{tabularx}

%\usepackage{placeins}

%\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}


\begin{document}
\title{Interface for Model Parallelism}
\author{\textbf{Author}\\
  Dr. Martin KÃ¼hn\\
 \\[\baselineskip]
     %  \textbf{Mit Beitr344gen von}\\
 }
\date{\today}
%\place{Kaiserslautern}

\maketitle

\allowdisplaybreaks

 \maketitle

 \begin{abstract} 
    
 \end{abstract}

\tableofcontents


\chapter{Definition}

\section{Input parameter}

$r$ is the rank of the current process.

$M=r_0, r_1, \dots$ is the full machine consisting of all the ranks. The list is ordered. $|M|$ is the number of ranks.

$D$ is the decompostion of a tensor over the ranks of the  machine
as follows
$\mathcal{N} \times \mathcal{C} \times \mathcal{H} \times \mathcal{W}$.

\chapter{Tarantella}

Tarantella should provide a function that returns a pointer to a buffer
in a GPI segment.

get\_segment\_buffer(size)

\chapter{Convolutionial layer}

The layer must have global parameters describing the global parameters.
These are $N, C, F, H, W$. $N$ is the batch size, $C$ the number of input
channels, $F$ the number of output channels or filters, $H$ the height and
$W$ the width of the images.

Additionally for a distributed convolutional layer we need to know the
following.

The own rank $r$ and the machine $M$.

The distribution of the input tensor over the dimensions
$\bar{\mathcal{N}} \times \bar{\mathcal{C}} \times \bar{\mathcal{H}} \times
\bar{\mathcal{W}}$. Here $\bar{\mathcal{N}}$ is not the number of samples
but the number of instances this dimenstion is distributed over and so on. So
$\bar{\mathcal{N}}=1$ means no distribution.

The distribution of the output tensor over the dimensions
$\hat{\mathcal{N}} \times \hat{\mathcal{C}} \times \hat{\mathcal{H}} \times
\hat{\mathcal{W}}$.

Maybe we want to give strides for distribution of the dimensions over the
ranks.
$\tilde{\mathcal{N}} \times \tilde{\mathcal{C}} \times \tilde{\mathcal{P}}$
or $\breve{\mathcal{N}} \times \breve{\mathcal{C}} \times \breve{\mathcal{P}}$.
Here stands
$ \tilde{\mathcal{P}}$ for $\tilde{\mathcal{H}} \times \tilde{\mathcal{W}}$.

Further we allow

\begin{eqnarray}
  \bar{\mathcal{H}} \times \bar{\mathcal{W}} & \leq & \tilde{\mathcal{P}}\\
    \bar{\mathcal{C}} \times  \bar{\mathcal{H}} \times \bar{\mathcal{W}}
  & \leq &
  \tilde{\mathcal{C}} \times  \tilde{\mathcal{P}}
  \\
     \bar{\mathcal{N}} \times \bar{\mathcal{C}} \times  \bar{\mathcal{H}} \times \bar{\mathcal{W}}
  & \leq &
   \tilde{\mathcal{N}} \times \tilde{\mathcal{C}} \times  \tilde{\mathcal{P}}
   \leq |M| \\
     \hat{\mathcal{H}} \times \hat{\mathcal{W}} & \leq & \breve{\mathcal{P}}\\
    \hat{\mathcal{C}} \times  \hat{\mathcal{H}} \times \hat{\mathcal{W}}
  & \leq &
  \breve{\mathcal{C}} \times  \breve{\mathcal{P}}
  \\
     \hat{\mathcal{N}} \times \hat{\mathcal{C}} \times  \hat{\mathcal{H}} \times \hat{\mathcal{W}}
  & \leq &
   \breve{\mathcal{N}} \times \breve{\mathcal{C}} \times  \breve{\mathcal{P}}
  \leq |M|
\end{eqnarray}

Later on we would need a parameter that defines distribution of the weights.
Currently we are distributing the images only, so the weights are copied and
synchronized always.


Further we would need resources like GPI segments and allreduces.

In summary we have

\begin{tabular}{l}
  $(N, C, F, H, W)$ \\
  $r$ \\
  $M$ \\
  $\bar{\mathcal{N}} \times \bar{\mathcal{C}} \times \bar{\mathcal{H}} \times
  \bar{\mathcal{W}}$\\
$\tilde{\mathcal{N}} \times \tilde{\mathcal{C}} \times \tilde{\mathcal{P}}$\\
  $\hat{\mathcal{N}} \times \hat{\mathcal{C}} \times \hat{\mathcal{H}} \times
  \hat{\mathcal{W}}$\\
  $\breve{\mathcal{N}} \times \breve{\mathcal{C}} \times \breve{\mathcal{P}}$\\
  (distribution pattern weights) \\
  GPI segmnts 
  allreduce double buffer
\end{tabular}

\section{Callback functions}

The ranks to synchronize the weights is returned by a callback function.

get\_ranks\_synch\_weights().

The size of the gradients is returned.

get\_sizeof\_gradients()

Tarantella has to create an allreduce (double buffer) for this layer if the
return of get\_ranks\_synch\_weights() is not empty. The size is
given by get\_sizeof\_gradients().

get\_sizeof\_input\_tensor()
get\_sizeof\_output\_tensor()

\section{Implementation}

Tarantella creates an allreduce of size get\_sizeof\_gradients() on the
ranks given by get\_ranks\_synch\_weights(). Tarantella delivers
pointers within GPI segments to store the local gradients in.

The implementation puts the local gradients directly into the segments.

The Tarantella function get\_segment\_buffer(size) is used to allocate a
buffer in a GPI segment. On the first call they exchange a pointer to
the exchange buffers with all their neighbors.

\subsection{Forward pass}
\begin{itemize}
\item
  start collecting all the input data needed
\item
  calculate convolution with local data
\item
  calculate convolution with remote data
\item
  fuse both data
\end{itemize}

\subsection{Backward pass}
\begin{itemize}
\item
  start collecting all the input data needed
\item
  calculate gradient weights with local data
\item
  calculate error signal with local data
\item
  calculate gradient weights with remote data
\item
  calculate error signal with remote data
\item
  fuse data
\end{itemize}

\chapter{Instance Normalisation}

The layer must have global parameters describing the global parameters.
These are $N, C, H, W$. $N$ is the batch size, $C$ the number of input
channels, $H$ the height and
$W$ the width of the images.

Additionally for a distributed convolutional layer we need to know the
following.

The own rank $r$ and the machine $M$.

The distribution of the input tensor over the dimensions
$\bar{\mathcal{N}} \times \bar{\mathcal{C}} \times \bar{\mathcal{H}} \times
\bar{\mathcal{W}}$. The dimension of the input tensor is the dimension
of the output tensor.

Maybe we want to give strides for distribution of the dimensions over the
ranks.
$\tilde{\mathcal{N}} \times \tilde{\mathcal{C}} \times \tilde{\mathcal{P}}$.
Here stands
$ \tilde{\mathcal{P}}$ for $\tilde{\mathcal{H}} \times \tilde{\mathcal{W}}$.

Further we allow

\begin{eqnarray}
  \bar{\mathcal{H}} \times \bar{\mathcal{W}} & \leq & \tilde{\mathcal{P}}\\
    \bar{\mathcal{C}} \times  \bar{\mathcal{H}} \times \bar{\mathcal{W}}
  & \leq &
  \tilde{\mathcal{C}} \times  \tilde{\mathcal{P}}
  \\
     \bar{\mathcal{N}} \times \bar{\mathcal{C}} \times  \bar{\mathcal{H}} \times \bar{\mathcal{W}}
  & \leq &
   \tilde{\mathcal{N}} \times \tilde{\mathcal{C}} \times  \tilde{\mathcal{P}}
   \leq |M|
\end{eqnarray}


Further we would need resources like allreduces.

In summary we have

\begin{tabular}{l}
  $(N, C, H, W)$ \\
  $r$ \\
  $M$ \\
  $\bar{\mathcal{N}} \times \bar{\mathcal{C}} \times \bar{\mathcal{H}} \times
  \bar{\mathcal{W}}$\\
$\tilde{\mathcal{N}} \times \tilde{\mathcal{C}} \times \tilde{\mathcal{P}}$\\
  allreduce double buffer weights
  allreduce double buffer norm
\end{tabular}

The allreduce double buffer norm is not part of the gradients update but
is used inside the layer to reduce the norm. Latency is critical here.

\section{Callback functions}

The ranks to synchronize the weights is returned by a callback function.

get\_ranks\_synch\_weights().

The size of the gradients is returned.

get\_sizeof\_gradients().

The ranks to synchronize the norm is returned by a callback function

get\_ranks\_synch\_norm().

The size of the norm is returned by

get\_sizeof\_norm().

Tarantella has to create two allreduces (double buffer) for this layer if the
return of get\_ranks\_synch\_weights() andget\_ranks\_synch\_norm()
are not empty. The size is
given by get\_sizeof\_gradients() and get\_ranks\_synch\_norm().

get\_sizeof\_tensor()

\section{Implementation}

\subsection{Forward pass}
\begin{itemize}
\item
  compute local norm
\item
  allreduce norm
\item
  perform scaling
\end{itemize}


\subsection{Backward pass}
Not sure if it works like this:

\begin{itemize}
\item
  calculate local gradient of norm
\item
  calculate local error signal of norm
\item
  allreduce local data
\item
  perform backward gradient of norm.
\item
  perform backward gradient of error signal
\end{itemize}

\chapter{Concat}



\end{document}
