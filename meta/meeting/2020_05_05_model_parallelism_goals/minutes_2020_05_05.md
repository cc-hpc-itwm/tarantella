## date: 05/05/2020
## time: 15.00 -- 17:00
## attendees:
  * Alexandra Carpen-Amarie [ACA]
  * Martin Kuehn [MK]
  * Peter Labus [PL]

## Status Meeting

* [PL] attended ICLR2020

### Coding guidelines

* Add to the [repo](https://gitlab.itwm.fraunhofer.de/carpenamarie/hpdlf/-/blob/master/meta/coding_style.md)

### Model parallelism

* **Distributing images and duplicating the weights** approach
    - details [here](https://gitlab.itwm.fraunhofer.de/labus/epigram-hs/-/blob/master/docs/pipelining/pipelining.pdf)

* Proposal: Combine Convolution with InstanceNorm/ReLu
    - might be issues with legacy codes which only use simple convolutions (switch to turn on/off?)
    - TF: how to replace an operator or a combination of ops?

* Implement distributed InstanceNorm vs. collect the batch results of the convolution and distribute across nodes and perform local InstanceNorm for micro-batches
    - distribute in the batch direction (data parallelism) for InstanceNorm
    - distributed the image across nodes for Convolution
    - synchronizes all ranks with an allreduce with almost no chance for overlapping
    
* MaxPooling 
    - Challenge: cutting the image (distribute it across nodes) such that the cut does not interfere with the pooling (which requires communication)
    - generic distributed implementation is not trivial

* Loss function
    - compute partial results within the image (local pixels) and aggregate them
    - if loss can't be aggregated aggregate the input of the loss
    - standard loss functions in segmentation 

* TF op implementations
    - MKL/DNNL/oneDNN wrappers 
    - Convolutions
        - exchange boundary pixels
        - would require slicing the Tensors or calling DNNL functions with strides

* Goals: distributed convolution implementation [MK]
    - split in the slowest image dimension
    - start defining interfaces for Python/C++
