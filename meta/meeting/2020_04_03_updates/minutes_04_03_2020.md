## date: 04/03/2020
## time: 10.00 -- 11.00
## attendees:
  * Alexandra Carpen-Amarie [ACA]
  * Martin Kuehn [MK]
  * Peter Labus [PL]

## benchmarks

* general notes:
  * local memory <-> segment copies, even if completely hidden are not "free", b/c the consume memory bandwidth
  * GPUs on a single rank share their network interface, so resource consumption is 4 times #ranks e.g. on GPU-Cluster,
    but resources still sufficent for resnet50
* looked again on existing bootstrap experiemnts on Cifar-10 for single rank performance:
  * large errors? trustworthy results?
  * TODO: calculate statistics & visualize

* use TensorBoard for future profiling
  * maybe collecting time stamps from SynchCommnicator later in the debuging process
  * search method to extract data semi automatically and text based from tensorbard to get easier to relevant results

## allreduce tests

* run CI with max number of processes available -> select number of ranks *within* tests (gaspi groups)
* allreduce butterfly tests stalls when # elements < # nodes

## merge of branch refactor

[MK] points out to previously raised objections on the merge of hpdlf branch "refactor" which happend one week ago. As the other team members fully support the merge a time consuming merge process + discussion is abandoned to prioritize more urgent matters
