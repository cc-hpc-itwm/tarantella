HP-DLF Strategy Meeting 2020 --- Meeting Minutes
======

## date: 01/15/2020
## time: 14.00 -- 18.00
## attendees:
  * Alexandra Carpen-Amarie [ACA]
  * Martin Kuehn [MK]
  * Peter Labus [PL]


## Meeting goal: What should be our development focus until 10/2020?

## Agenda
1. review: project proposal
2. review: alternative products
3. What should be our unique selling point / motivation / incentive?
4. What feature(s) should we focus on in 2020?

# Review: project proposal
A list of bullet points (PFA) extracted from the original proposal by PL has been review.

# Review: alternative products
The following (incomplete) list of alternative frameworks and approaches towards high-performance deep learning
has been identified:

* Model parallel:
  * Mesh Tensorflow [Google Brain, 1811.02084, https://github.com/tensorflow/mesh] -> TensorFlow
  * TF Replicator [Google Deepmind, 1902.00465, part of tf.distribute.Strategy] -> TensorFlow
  * LBANN [LLNL, https://lbann.readthedocs.io/en/latest/publications.html] -> inspired by PyTorch API
  * FlexFlow [Stanford, Microsoft, PMLR2018: Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks, https://arxiv.org/pdf/1802.04924.pdf]
* Pipeline:
  * PipeDream [Microsoft Research, 1806.03377, https://github.com/msr-fiddle/pipedream] -> PyTorch
  * GPipe [Google Brain, 1811.06965, https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py, also cf. https://github.com/tensorflow/lingvo/issues/112] -> TensorFlow, PyTorch (https://github.com/kakaobrain/torchgpipe)
* Data parallel (distributed):
  * Horovod [Uber, 1802.05799, https://github.com/horovod/horovod] -> TensorFlow, Keras, PyTorch, Apache MXNet
* Communication libraries:
  * NCCL [NVidia, https://github.com/nvidia/nccl]
        * collectives for GPUs
        * Allreduce, Reduce, Broadcast, Allgather etc.
        * implementation based on ring algorithms
  * Gloo [Facebook, https://github.com/facebookincubator/gloo]
        * collectives for CPUs, GPUs
        * Allreduce, Reduce, Broadcast, Allgather etc.
        * mutiple algoritms for each collective, CUDA-aware, overlapping
  * Blink [Microsoft Research, UC Berkeley, 1910.04940]
        * comm library for heterogeneous networks for GPUs
        * Allreduce, Broadcast
        * NCCL API
  * Tensorflow tf.distribute module [https://www.tensorflow.org/api_docs/python/tf/distribute]
        * collectives for GPUs
        * Allreduce, Reduce, Broadcast
  * Aluminum [LLNL (LBANN), https://ieeexplore.ieee.org/document/8638639]
        * collectives for GPUs
        * Allreduce, Allgather, Reduce-scatter, Reduce
        * blocking and non-blocking Allreduce (Rabenseifner alg), support for multiple, concurrent Allreduce ops
        * based on NCCL, MPI
  * BlueConnect [IBM, SysML2019, https://mlsys.org/Conferences/2019/doc/2019/130.pdf]
        * hierarchical (topology-aware) Allreduce for GPUs
        * based on MPI and NCCL
  * Baidu Allreduce [https://github.com/baidu-research/baidu-allreduce]
        * Ring allreduce implementation for GPUs
        * based on MPI
* single GPU optimization [MSR Gist] -> reduce memory impact on single GPU
* data loading optimzation [MSR: THEMIS '20, The Case for Unifying Data Loading in Machine Learning Clusters '19]
* other optimizers

A good recent overview can be found here: 1903.11314, and references therein.

# What should be our unique selling point / motivation / incentive?

Starting from the technical desiderata and other goals stated in the original proposal
we identified the following high-level goals:

* be able to execute larger models conveniently with reasonable performance (hiding HPC complexity from Deep Learning user)
  * usability: automatic partitioning/execution of model on available hardware, possibly using hybrid approaches
  * performance: reasonable single-node performance & scaling on multi-node [in terms of time to accuracy]
  * execution on various types of hardware (CPUs, GPUs) possible

We identified the following reasons, why we should care about large models:
* project partners care (in particular Heidelberg)
* our HPC group cares (seismic)
* other science groups potentially care (namely physicians/neuro-scientists in Garching/Juelich)
* part of original proposal

However, we identified the following challenges:
* performance: partitioning depends on the network, less overlapping potential
* usability: may be more of a software development task (-> enough expertise?)

We identified the following alternative aims for the further development of HP-DLF:
* optimizing data parallelism (-> wouldn't deliver all the goals in proposal, e.g. larger models)
* combine pipelining parallelism / model parallelism w/o automatic partitioning (goal: scalability, issue: unique selling point/usability)

We identified the following open research questions that might be worth addressing in the future:
* Is there an I/O bottleneck and can we improve upon it? (massive data parallelism with caching in global memory layer/RAM, such as the virtual memory layer in GSPC)

We also found a number of desirable steps of lower priority, which could be addressed in 2021:
* supporting other frameworks (PyTorch, MXNet)
* new optimizers

# What feature(s) should we focus on in 2020?
Given the goal of "execute larger models conveniently with reasonable performance" implementing
pipeline-parallelism combine with the already existing (distributed) data parallelism seems reasonable.

A final conclusion has not been reached. To that end, we have scheduled a follow-up meeting 01/17/2020, 15.00 -- 17.00.
