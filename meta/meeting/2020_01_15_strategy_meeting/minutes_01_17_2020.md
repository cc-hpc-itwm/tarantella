HP-DLF Strategy Meeting 2020 Part 2 --- Meeting Minutes
======

## date: 01/17/2020
## time: 15.00 -- 17.00
## attendees:
  * Alexandra Carpen-Amarie [ACA]
  * Martin Kuehn [MK]
  * Peter Labus [PL]


## Meeting goal: What should be our development focus until 10/2020?

## Alternative goals

Additionally to the proposals from 01/15/2020 it has been proposed by MK to focus on full model parallelism
targeting convolutions and U-Nets on huge image datasets that are not executable with pipelining parallelism.

The proposal to focus on questions of I/O in Deep Learning will be addressed in the DeToL project.

### Details on the "Full model parallelism" direction

* Goal: execute very large models fully model parallel, e.g. U-Net with 512^3 on 64 GPUs
* Second goal is decent performance

* Pros:
    - No automatic distribution of the net is needed
    - We exploit our core competences, GPI, overlapping, zero copy, etc.
    - Heidelberg partners can do things that nobody did before
    - Minimizing memory usage, enabling support for medical images and model parallelism were all mentioned in the proposal
    - Having support for large 3D images might be useful for our future seismic projects.
* Downsides:
    - Existing approaches already support model parallelism: tf.replicator, LBANN, FlexFlow
    - Less potential for extending the project
    - Less potential to attract new/other users

* Using Caffe
    - Pros:
        - much smaller and propably more accessible than TensorFlow
        - Caffe is mentioned in the proposal (but so is TensorFlow)
    - Cons:
        - we can spend the time learning about TensorFlow for building expertise regarding a more widely-used framework, which is a skill that we might need/use in future projects

## What should be our main focus/selling point?
* Provide an easy to use, convenient framework for users
    - Even if we don't achieve significant performance improvements over competitors, it can put us on the map of existing solutions
    - Potential for obtaining future funding/projects to further develop the framework
    - Performance is important, but not the main goal
* Alternatively, our niche could be performance optimization for deep learning
    - We can bring HPC expertise
    - Focus on performance improvement
    - Make HPC optimization techniques available/known to the deep learning community

## What should be our target audience?
Depending on what audience to target, the development focus might shift.
* Production - performance/cost minimization is essential
* Research - usability, researchers might work on very large models (even without large amounts of data)

We agreed to develop with the latter audience in mind.

## What feature(s) should we focus on in 2020?
We agreed to pursue the following targets in 2020:
* Build a framework that can easily integrate pipelining and later model parallelism
* Potential steps
    - pipelining in combination with data parallelism
    - layer distribution
    - evaluation with at least one large model
        - E.g., Unet3d and Transformer models
        - such tasks can be delagated to master students and HiWis
