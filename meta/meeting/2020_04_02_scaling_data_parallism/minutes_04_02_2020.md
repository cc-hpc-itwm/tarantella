## date: 04/02/2020
## time: 13.30 -- 14.45
## attendees:
  * Alexandra Carpen-Amarie [ACA]
  * Martin Kuehn [MK]
  * Peter Labus [PL]

## benchmark results

GPU-cluster (Styx):
* ResNet50 on ImageNet

* Taranetella: from 1 GPU to 16 GPUs (= 4nodes x 4GPUs)
* Horovod: 1x4 only
* **Horovod is significantly better than us**
* two problems:
  * overhead on single node (almost 50% on GPUs) 
  * deviation from linear speedup when increasing #GPUs

Overhead using 1rank: Tarantella vs No Tarantella: [time per iteration]

GPU Titan V
* with tarantella:    513ms stddev 30ms stddev average < 0.1ms
* without tarantella: 304ms stddev 5ms  stddev average < 0.1ms
* diff: 209ms


Seislab SKX gen2 single socket
* with tarantella:     4227ms stddev 40ms stddev average < 5ms
* without tarantella:  4043ms stddev 40ms stddev average < 5ms
* diff: 184s


Overhead using 16 ranks vs 1x2 ranks: [time per iteration]
* GPU Titan V        119ms (4x4 ranks) vs (1x2 ranks)
* Seislab SKX gen2   103ms (8x2 ranks) vs (1x2 ranks)

Iteration Runtime, 1 rank without Tarantella
* GPU      304 ms
* Seislab SKX gen2 4043 ms

## Model specs and resources needed

See https://gitlab.itwm.fraunhofer.de/labus/gpionnx_experiments/-/tree/master/tensorflow/resnet50

Bottomline:
There are enough network and memory bandwidth for perfect overlapping of communication and computation for high number of ranks.


* TODO: investigate reasons for single node overhead [@MK]
* possible reasons:
  * using TF ops vs python functions in nodes of TF graph
  * having a single wait_all in TF graph
